### Title: “Data Mining H1”

### Date: “2023-02-17”

### Author: “Qin Xia, Wenhan Li, Yufeng Hu”

### Saratoga house prices

lm\_medium baseline RMSE:

    ## [1] 61706.55

Best LM model RMSE:

    ## [1] 54229.81

KNN output with optimal K:

    ##    k      RMSE  Rsquared      MAE     RMSESD RsquaredSD      MAESD
    ## 1 12 0.6255762 0.6106535 0.438029 0.07945497  0.1008205 0.03819406

We find that linear model does better at achieving lower out-of-sample
mean-squared error. The KNN model provides a poorer fit to the data. The
true relationship between price and those variables we choose is closer
to linear. First, we set up 20 folds and use cross validation method.
And we loop over k-fold cross validation linear model to choose the best
model (without any interactions) with lowest RMSE. Then adding
interaction variables to best model to see if it improves RMSE. When
choose KNN model, we decide to use all variable in all models, and we
set the k value of the KNN model from 1 to 50. Because after repeated
testing, the optimal k value never exceeds 10 Comparing standardized KNN
model’s and linear model for their RMSE. We find that linear model
performs better. In the optimal linear model that we chose, there are
several variables are very significant. They are age, livingarea,
bedroom, bathroom, rooms, fueloil, waterfront, newconstruction and
centralair. And the effect of livingarea measured by the linear model is
quite small. Age has a significant negative effect on housing prices.
The influence of house age on house value is mainly reflected in the
value growth space, and the old house will gradually face the problem of
construction depreciation. There is a negative correlation between the
number of bedrooms and house prices. Because more bedrooms mean less
square footage per room, given the same size. Most people would not like
a house with lots of small rooms. There is a positive correlation
between the number of bathrooms and house prices, because bathrooms are
relatively small, and most houses do not have more than three bathrooms.
The new construction house may take time for further development. And
there may be potential problems. So the house price is going to be lower
than other houses. Waterfront houses are going to cost a lot more than
the average house.

### Classification and restrospective sampling

![](H2_files/figure-markdown_strict/unnamed-chunk-4-1.png)

    ## 
    ## Call:
    ## glm(formula = Default ~ duration + amount + installment + age + 
    ##     history + purpose + foreign, family = "binomial", data = german_credit)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -2.3464  -0.8050  -0.5751   1.0250   2.4767  
    ## 
    ## Coefficients:
    ##                       Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept)         -7.075e-01  4.726e-01  -1.497  0.13435    
    ## duration             2.526e-02  8.100e-03   3.118  0.00182 ** 
    ## amount               9.596e-05  3.650e-05   2.629  0.00856 ** 
    ## installment          2.216e-01  7.626e-02   2.906  0.00366 ** 
    ## age                 -2.018e-02  7.224e-03  -2.794  0.00521 ** 
    ## historypoor         -1.108e+00  2.473e-01  -4.479 7.51e-06 ***
    ## historyterrible     -1.885e+00  2.822e-01  -6.679 2.41e-11 ***
    ## purposeedu           7.248e-01  3.707e-01   1.955  0.05058 .  
    ## purposegoods/repair  1.049e-01  2.573e-01   0.408  0.68346    
    ## purposenewcar        8.545e-01  2.773e-01   3.081  0.00206 ** 
    ## purposeusedcar      -7.959e-01  3.598e-01  -2.212  0.02694 *  
    ## foreigngerman       -1.265e+00  5.773e-01  -2.191  0.02849 *  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 1221.7  on 999  degrees of freedom
    ## Residual deviance: 1070.0  on 988  degrees of freedom
    ## AIC: 1094
    ## 
    ## Number of Fisher Scoring iterations: 4

From the regression summary, we see that credit history has a positive
correlation with the prediction of whether someone will default. Since
we selected a GLM model, the coefficients are powers of e, meaning that
the coefficient of historypoor is roughly 0.3302 while the coefficient
of historyterrible is roughly 0.1518. This contradicts the facts since
it suggests that poor credit history is less likely to default than
terrible credit history, making this model not suitable for screening
prospective borrowers to classify them into “high” versus “low”
probability of default. This is probably caused by the fact that the
data is substantially oversampled of defaults, relative to a random
sample of loans in the bank’s overall portfolio, as a result of the data
collection design. As such, we would suggest that the bank adopt a
random sampling data collection design to more accurately portray the
true proportion of each credit history level when conducting regression
analysis.

### Children and hotel reservations

#### Model building

Baseline 1 RMSE, training confusion matrix, and testing confusion
matrix.

    ## [1] 0.25686

    ##    yhat_bl1_train
    ## y       0
    ##   0 33022
    ##   1  2978

    ##    yhat_bl1_test
    ## y      0
    ##   0 8343
    ##   1  657

Baseline 2 RMSE, training confusion matrix and accuracy, and testing
confusion matrix and accuracy.

    ## [1] 0.2247717

    ##    yhat_bl2_train
    ## y       0     1
    ##   0 32552   470
    ##   1  1913  1065

    ## [1] 0.9338056

    ##    yhat_bl2_test
    ## y      0    1
    ##   0 8226  117
    ##   1  420  237

    ## [1] 0.9403333

Own lm RMSE, testing confusion matrix, and testing accuracy.

    ## hotel_train

    ## Warning in predict.lm(model, data): prediction from a rank-deficient fit may be
    ## misleading

    ## [1] 0.2218346

    ## Warning in predict.lm(lm3, hotel_test): prediction from a rank-deficient fit may
    ## be misleading

    ##    yhat_3
    ## y      0    1
    ##   0 8223  120
    ##   1  404  253

    ## [1] 0.9417778

The base line 1 model has an out-of-sample accuracy of 0.9243333, as it
predicted all the bookings in the test set to be without children. The
high value doesn’t justify for the accuracy for the model as this was
the original proportion of no-children-bookings. This can be considered
as a null model performance on test set.

The second base line model has an out-of-sample accuracy of 0.9375556,
it correctly predicted 249 out of 432 bookings that are with kids, and
falsely predicted 130 out of 8189 no-kid bookings to be with kids.
There’s a 1.014 relative improvement for the second base line model.

We hand-picked various features and interactions and eventually decided
the above model as our final linear model, which yields by far the
lowest rmse. The confusion matrix showed a 0.94 accuracy for
out-of-sample prediction, which beat the second base line model by an
absolute improvement of around 0.002. This is acceptable considering the
null model performance.

#### Model Validation: step 1

![](H2_files/figure-markdown_strict/unnamed-chunk-8-1.png)

#### Modeling validation: step 2

    ##    predicted probability observed probability  accuracy
    ## 1             0.04400000            0.1040000 0.9080000
    ## 2             0.04000000            0.0800000 0.9200000
    ## 3             0.03600000            0.0680000 0.9520000
    ## 4             0.03200000            0.0480000 0.9520000
    ## 5             0.07200000            0.1000000 0.9320000
    ## 6             0.04800000            0.0960000 0.9280000
    ## 7             0.02800000            0.0680000 0.9440000
    ## 8             0.04800000            0.1000000 0.9320000
    ## 9             0.04800000            0.0720000 0.9680000
    ## 10            0.03600000            0.0760000 0.9520000
    ## 11            0.04800000            0.0760000 0.9400000
    ## 12            0.06400000            0.0760000 0.9400000
    ## 13            0.08400000            0.1000000 0.9360000
    ## 14            0.03200000            0.0720000 0.9520000
    ## 15            0.03600000            0.0720000 0.9320000
    ## 16            0.04800000            0.0960000 0.9200000
    ## 17            0.06024096            0.1044177 0.9236948
    ## 18            0.05600000            0.0720000 0.9120000
    ## 19            0.02800000            0.0600000 0.9520000
    ## 20            0.06800000            0.0680000 0.9280000

The matrix summed up our validation results, with the first column
showing the predicted probability in each fold. The total predicted
number of bookings with children will be the product of this probability
and the number of observations in the fold. The second row is the
actual(observed) probability of bookings with children, since the number
of observations within each fold is the same for predicted and observed,
we can make direct comparison. In general, as are shown in the third
column, our best model from part 1 is performing at these accuracy
levels. The mean accuracy is at 0.9346. Due to the slight margin of
outperformance and the change of data, it is reasonable that the
advantage of our best model can be insignificant now in comparison with
the second base line, which requires further examination. Nevertheless,
there is still visible improvement from the null model.
